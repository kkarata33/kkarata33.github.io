---
layout: post
title: Blog Post 2
---

In this blog post, we will write a tutorial on a simple version of the spectral clustering algorithm for clustering data points.

#### Notation

In all the math below:
- boldface capital letters like $$A$$ refer to matrices (2d arrays of numbers).
- Boldface lowercase letters like  ùêØ refer to vectors (1d arrays of numbers).
- AB refers to a matrix-matrix product (`A@B`). Av refers to a matrix-vector product (`A@v`).

### ¬ß1. Intro

If you are like me, you may be thinking, what is this foreign term, *spectral clustering*? To break it down to basics, spectral clustering is a fancy term for sorting data points into different categories, except it is more advanced than just plain clustering. Similar to kMeans, Spectral clustering is a method we can use for sorting more complex data, aka data that is not necessarily in circular clusters.

To start, let's look at an example using kMeans to sort our data into different categories:

```python
import numpy as np
import matplotlib as plt
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```
We go from this:

![Blobs]({{site.baseurl}}/images/blobs.png)

```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
To this using kMeans:

![KMeans Blobs]({{site.baseurl}}/images/kmeansblobs.png)

Not bad! But what if our data is shaped weird like this?

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

![Making Moons]({{site.baseurl}}/images/moons.png)

Here we can distinctly see the two cluster shaped blobs, but kMeans has trouble sorting these points because the clusters are not circular.

```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![Kmeans Moons]({{site.baseurl}}/images/kmeansmoons.png)

This is where kMeans will only get you so far when attempting to identify different clusters. So, lets try our hand at Spectral Clustering!

### Lets Get Started

First let's import all the packages we will need

```python
import numpy as np
from plotly import express as px
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt # optional, in case we want to plot
```

## Part A: Constructing a Similarity Matrix, A

Since methods like KMeans wont work with data shaped like this, we want to distinguish the moon-shaped clusters by finding points that are close to one another rather than trying to identify the center of a cluster, as KMeans does.

Constructing a similarity matrix `A` will allow us to do this by detailing the relationship of one point to others and show us the number of connections each point in a network of points has. Similarity matrices consist of 0s and 1s, and in this case, a 1 will represent that a node is connected to another and a 0 will represent the two nodes are not connected.
In order to determine if two nodes are connected, however, we will have to implement an arbitrary threshold that filters points into two categories (0 and 1) based on their distances to one another. Here, `epsilon` is that threshold. And we will set `epsilon` to `0.4` We also have to calculate *pairwise distances* for each pair of points to determine whether the distances between each other are less than `epsilon`. Luckily, sklearn has a handy function that allows us to calculate these distances.

**Note: The diagonal entries `A[i,i]` will all be equal to zero because the distance between a point and itself is 0.**

```python
epsilon = 0.4

A = 1.0*(sklearn.metrics.pairwise_distances(X) < epsilon) #if point is within distance epsilon, it is denoted as 1
np.fill_diagonal(A, 0) #fills the diagonal with values of A and everything else is 0
```
Think of each row as representing a point, and each column representing a point. These columns show whether a two points are within distance epsilon from each other or not.

## Part B

Now, after constructing our similarity matrix, our next question to tackle is:

>How do we optimize our model so that we can "cut" our data points into two approximately equal sized groups and cut as few connections between the points as possible?

In other words, we want to *minimize* the number of connections being cut because we want to make as accurate cluster predictions as possible. We also want to ensure that the two groups are roughly equal so that our model doesn't accidentally characterize one point, for instance, as one cluster, and the other 100 points as another cluster.

To do this, we want calculate the *binary norm cut objective* of matrix `A`:

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

Here,
- $$\mathbf{cut}(C_0, C_1)\left$$ indicates our cut, or how many cuts we need to separate them, of clusters `C_0` and `C_1`
- $$\mathbf{vol}(C_0)$$ is the size of the cluster

Again, since we want to minimize the number of connections cut, we want `N(C0, C1)` to be small.

#### B.1 The Cut Term

Lets make a function to compute our cut term, `cut(C_0, C_1)`:

To do this, we will iterate through each row and column in A, which is a 200x200 matrix, and check if the values in y[row] and y[column] match (either 0 or 1). If the points in y don't match, you then precede to check whether A at that index is equal to 1. If it is, it signifies that the point connections don't match (i.e they are in different clusters) and we add 1 to the cut term.

```python
def cut(A, y):

    cut = 0

    for row in range(A.shape[0]):
        for col in range(A.shape[1]):
            if y[row] != y[col]: #if the points in y dont match, check the value of A at that index
                    cut += 1


    return cut
```
```python
cut(A, y)
```
```python
26
```

This cut is 26. Not bad! This means that out of all the connections between points, only 26 of them have been cut. This is good, because we want to minimize the number of connections being cut in order to create a more accurate cluster prediction.

To check that the cut objective performs better for true clusters rather than random ones, we will calculate the cut objective for a random matrix of data of 0s and 1s. The resulting output/number of cuts for the true data should be a lot smaller than for the random data.

```python
vector = np.random.randint(0, 2, size = n)

K = cut(A, vector)
K
```
```python
2232
```
The cut for randomly generated data is 2232, which is much higher, signifying over 2000 connections will be cut. This makes sense, because we are using random data instead of our true data (which has clusters).

#### B.2 The Volume Term

The other term in our normcut objective was the *volume term*. The volume term denotes the size of the respective cluster. In other words, the *volume* of cluster `C_0` is a measure of how "big" cluster `C_0` is.

Remember, the binary normcut objective asks us to find clusters `C_0` and `C_1` so that:

1. We minimize the number of connections cut that join `C_0` and `C_1`.
2. `C_0` and `C_1` are approximately the same size.

To start, the volume term protects against the solution where cluster 1 has all the points and cluster 0 has none. In other words, it tries to maintain a relatively balanced number of points per cluster. The cut term, as we saw above, was the number of data points that were sorted into different clusters C1 and C0. Here, we are calculating the size of the cluster, which is how many pairs of points are both in that specific cluster AND close together.

So let's create a volume function that computes the sizes/volumes of each cluster `C_0` and `C_1` and returns their respective volumes as a tuple

```python
def vols(A, y):
    C0 = np.where(y == 0)[0] #create an array of all indicies where y == 0
    C1 = np.where(y == 1)[0]

    v0 = A[C0].sum() #sum each row of A according to C0 index
    v1 = A[C1].sum()

    return(v0, v1)
```
Now we can rewrite the normcut formula using the vols function and cut function we previously defined:
```python
def normcut(A, y):

    (v0, v1) = vols(A, y)
    c = cut(A, y)

    return (c*(1/v0+1/v1))
```
```python
vols(A, y), vols(A, vector)
```
```Python
((2299.0, 2217.0), (2064.0, 2452.0))
```
```python
(normcut(A, y), normcut(A, vector))
```
```Python
(0.02303682466323045, 1.991672673470162)
```

As you can see, The normcut values according to the actual data compared to randomly generated points are much lower, indicating that less data is being 'cut' from the original cluster group.

## Part C

As you can see, the normcut objective that we defined above is small when the two clusters are both similar sizes and when the clusters don't overlap too much.

In order to determine which points are in `C_0` and `C_1`, we will define a new vector `Z` such that:

$$
z_i =
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\
\end{cases}
$$

To start, we will define a `transform` function, which takes in a matrix of data `A` and its `y` target data and computes a `z` vector. We will use the `vols` function that we previously defined to get the respective cluster sizes and then determine which operation to perform on each data point based on its corresponding y value. For instance, if y is equal to 0, then we will use `1/v0`. However, if the corresponding `y` value is one, we will use negative volume notation. This will help us identify which points are in which cluster.

```python
def transform(A, y):

    (v0, v1) = vols(A, y)

    z = 1/v0*(y == 0) + -1/v1*(y == 1) #if element in y is equal to 0, use the positive volume notation. Otherwise, use - 1/v1

    return z
```

Here we will check that z contains roughly as many positive as negative entries by checking that the identity of `zD1 = 0` is the vetor of `n` ones.

```python
z = transform(A, y)
k = np.sum(A, axis=1) #summing along rows of A

D = np.diag(k) #diagonal of k
```

```python
np.isclose(0, z.T@D@np.ones(n)) #checks that the identity is close to zero
```
```Python
True
```

Cool! This checks out!

We will also check if a is close to b by relating matrix product (a) to normcut objective (n) by computing each side and checking that they are equal.

```python
# matrix with shape nxn
D = np.zeros((n,n))
np.fill_diagonal(D, A.sum(axis = 1))# Fill diagonal of D with the ith row sum of A
z = transform(A,y)
a = 2 * (z.T @ (D-A) @ z) / (z.T @ D @ z) # calculate best clustering
n = normcut(A,y) #normcut objective
```
```python
np.isclose(a,b)
```
```python
True
```
Awesome!

## Part D

In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

 We can integrate this into our optimization by substituting for `z`, the orthogonal complement of `z` relative to `D1`. Fortunately, scipy.optimize has a minimizing function that we will use to minimize the function `orth_obj` with respect to `z`.

```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n)

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

As you can see here, we use the scipy.optimize function to minimize with respect to z.

```python
import scipy.optimize

z_min = scipy.optimize.minimize(orth_obj, np.random.rand(n)).x
```

## Part E

Now that we obtained the optimal value of z_min, we can create a set of binary labels to reflect the different clusters and we will plot so that points `z_min[i] < 0` are one color and points `z_min[i] >= 0` are another! Remember, the value of `z_min[i]`

Recall that, by design, only the sign of  actually contains information about the cluster label of data point `i`. Plot the original data, using one color for points such that  and another color for points such that .


```python
color = 1*(z_min < 0)

plt.scatter(X[:,0], X[:, 1], c = color)
```

This looks pretty good! However, one tradeoff is that this method is really slow to practically use. Now let's try using another method to cluster that is shorter and uses eigenvalues/eigenvectors of matrices instead. This method is a lot shorter and more efficient than optimizing the orthogonal objective!

## Part F

Remember we wanted to minimize this function with respect to `z`, subject to the condition `z^TD1 = 0`.

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

The Rayleigh-Ritz Theorem states that the minimizing $\mathbf{z}$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Why is this helpful? Well, `1` is actually the eigenvector with smallest eigenvalue of the matrix `D^-1(D-A)`

> So, the vector `z` that we want must be the eigenvector with  the *second*-smallest eigenvalue.

We will construct the matrix `L = D^-1(D-A)`, or the (normalized) *Laplacian* matrix, of the similarity matrix `A`. Then we will find the eigenvector corresponding to its second-smallest eigenvalue, naming it `z_eig`. Extracting it is tricky, so we will make use of the handy argsort() function to order the values of the matrices with indices indicating low to high values. Finally, we will plot the data again, using the sign of `z_eig` as the color.

```python
L = np.linalg.inv(D)@(D - A)

eigvalue = np.linalg.eig(L)[0]
eigvector = np.linalg.eig(L)[1]

idx = eigvalue.argsort()[1] #index of the second smallest element

z_eig = eigvector[:, idx] #getting the corresponding eigenvector
plot = z_eig < 0 #plotting based on positive and neg values
```

```python
plt.scatter(X[:,0], X[:, 1], c = plot)
```

We get basically the same result, and it was much quicker.

## Part G

Let's put it all together now. We will define a function that constructs a similarity matrix, then a Laplacian matrix, and it will also compute the eigenvector with the second-smallest eigenvalue of the Laplacian matrix, returning labels based on this eigenvector (0s or 1s).

This is basically an aggregation of most things we did above, so we will essentially just copy-paste with a few minimal changes.

```python
def spectral_clustering(X, epsilon):
    """
    Takes in data X in a matrix, and a float epsilon, which determines the threshold for the cluster
    Constructs a similarity matrix, a Laplacian matrix, and an eigenvector with the second smallest
    eigenvalue of the Laplacian matrix.
    Returns labels based on this eigenvector encoded into either 0 or 1 based on if the data point in in cluster
    C1 or C0
    """
    S = 1.0*(sklearn.metrics.pairwise_distances(X) < epsilon) #if point is within distance epsilon, it is denoted as 1
    np.fill_diagonal(S, 0)

    k = np.sum(S, axis=1) #summing along rows of S
    D = np.diag(k) #fills diagonal of D with row sums from k

    L = np.linalg.inv(D)@(D - S) #computes a Laplacian matrix

    eigenvalue, eigenvector = np.linalg.eig(L)[0], np.linalg.eig(L)[1] #stores the eigenvalues and eigenvectors in separate matrices

    idx = eigenvalue.argsort()[1] #index of the second smallest eigenvalue

    z_eig = eigenvector[:, idx] #getting the corresponding eigenvector

    return 1*(z_eig < 0) #if the eigenvector values are less than zero, they are in the cluster C1, indicated by a 1.
    #otherwise, they are in cluster C0, indicated by 0
```

### To summarize a bit what we did here:

*When using Spectral Clustering, the big picture question is:*

> Where can I make a cut in the network to separate into roughly EQUAL SIZE (volume) clusters that CUTS AS FEW CONNECTIONS AS POSSIBLE? (minimization problem)

#### A Matrix:
- Similarity Matrix A contains point connection information. It represents whether certain points are connected (1) or not (0).
  - There are 0s across the diagonal to demonstrate that points cannot be connected to themselves.
  - There is symmetry across diagonal signifying that points are connected reciprocally (e.g. 2 is connected to 4 and 4 is connected to 2)
  - Summing across rows represents the total number of connections from a single point.
- Epsilon represents how close each point is to each other (Similarity Matrix A tells us whether points are within distance epsilon to each other or not)

#### D matrix:
- Holds row sum info (number of connections per row) across diagonal.

#### L matrix:
- D-A row sums are zero.
- There is a distribution of each connection across diagonal (if 2 is in the diagonal row, indicates that 2 connections are distributed among rows)
- Makes diagonals all 1.
  - This is good because it shows where clusters are concentrated in the matrix and whether the clusters are well defined. If the top right and bottom left quadrants are mostly 0s, indicates that clustering is well defined. In other words, the interaction between C0 and C1 should be about equal to 0.

#### Eigenvalues and Eigenvectors
- Second smallest eigenvalue and corresponding eigenvector are found and are used to filter data points into clusters C0 and C1.

## Part H

Lets experiment with the dataset `make_moons`!

```python
np.random.seed(1435)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.15, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

![Make Moons]({{site.baseurl}}/images/bluemoons.png)

Now let's test it out!

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.25))
````
![Using Spectral Clustering on Moons]({{site.baseurl}}/images/bluemoonssc.png)

It looks like increasing the noise increases the dispersion of the data points, with larger noise values corresponding to less structured-clusters. As the noise increases, the spectral clustering method seems to have some trouble distinguishing between the two groups, but overall not bad!

## Part I

Now lets try our spectral clustering function on another data set -- the bull's eye!

```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```
![Make Circles]({{site.baseurl}}/images/bluecircles.png)

There are two concentric circles. As before, k-means will not do well here at all.

```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![KMeans Circles]({{site.baseurl}}/images/kmeanscircles.png)

Let's see if our function can successfully separate the circles.

It looks like for all epsilon values other than 0.3, 0.31, and 0.2, spectral clustering cannot accurately distinguish the clusters. However, these three values of epsilon were successful in separating the circles!

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.3))
```
![Spectral Clustering Circles]({{site.baseurl}}/images/sccircles0.3.png)

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.2))
```
![Spectral Clustering Circles]({{site.baseurl}}/images/sccircles0.2.png)

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.31))
```
![Spectral Clustering Circles]({{site.baseurl}}/images/sccircles0.31.png)

Amazing! With Spectral Clustering we were able to somewhat accurately identify separate clusters of non-circular shapes. By utilizing eigenvectors and eigenvalues, we were able to quickly identify the cut that optimizes both the size (volume) of clusters that cuts as few connections as possible, thereby accurately identifying clusters in noncircular formats.

{::options parse_block_html="true" /}
<div class="got-help">
Upon suggestion, I was able to simplify my cut function by getting rid of an unnecessary if statement, because incrementing the cut term by zero doesn't do anything (so I didn't need to filter by an additional condition). I also added more commentary on the performance of the function in part H with different levels of noises.
</div>

{::options parse_block_html="false" /}
{::options parse_block_html="true" /}
<div class="gave-help">
I helped others simplify their approach to constructing a similarity matrix in part A by combining the steps where you calculate pairwise distances and then multiply by 1 to get binary labels. I also encouraged others to write more about the concepts and they 'why' behind each section rather than just copy-paste the professors code.
</div>
{::options parse_block_html="false" /}
