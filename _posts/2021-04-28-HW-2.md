---
layout: post
title: Blog Post 2
---

In this blog post, we will write a tutorial on a simple version of the spectral clustering algorithm for clustering data points.

#### Notation

In all the math below:
- boldface capital letters like A refer to matrices (2d arrays of numbers).
- Boldface lowercase letters like  ùêØ refer to vectors (1d arrays of numbers).
- $\mathbf{A}\mathbf{B}$ refers to a matrix-matrix product (`A@B`). $\mathbf{A}\mathbf{v}$ refers to a matrix-vector product (`A@v`). 

### ¬ß1. Intro

In this problem, we'll study *spectral clustering*. Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure. To start, let's look at an example where we *don't* need spectral clustering. 

### Lets Get Started

First lets import all the packages we will need

```python
import numpy as np
from plotly import express as px
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt # optional, in case we want to plot
```

```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```
<!-- {% include blobs.html %} -->

*Clustering* refers to the task of separating this data set into the two natural "blobs." K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these: 

```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
### Harder Clustering

That was all well and good, but what if our data is "shaped weird"? 

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```
<!-- 
{% include moons.html %} -->

We can still make out two meaningful clusters in the data, but now they aren't blobs but crescents. As before, the Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`. Now k-means won't work so well, because k-means is, by design, looking for circular clusters. 

```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

<!-- {% include moons2.html %} -->

Whoops! That's not right!
As we'll see, spectral clustering is able to correctly cluster the two crescents. In the following problems, you will derive and implement spectral clustering.

### Part A: Constructing a Similarity Matrix, A

We are going to construct a similarity matrix, using parameter `epsilon` as our threshod for cluster identification. Entry `A[i,j]` should be equal to `1` if `X[i]` (the coordinates of data point `i`) is within distance `epsilon` of `X[j]` (the coordinates of data point `j`). 

**The diagonal entries `A[i,i]` should all be equal to zero.** We are going to use epsilon = 0.4

This similarity matrix will detail the relationship of one point to others and show us the number of connections each point in a network of points has. Think of each row as representing a point, and each column representing a point. These columns show whether a two points are within distance epsilon from each other or not.

```python
epsilon = 0.4

A = 1.0*(sklearn.metrics.pairwise_distances(X) < epsilon) #if point is within distance epsilon, it is denoted as 1
np.fill_diagonal(A, 0) #fills the diagonal with values of A and everything else is 0
```

### Part B

The matrix `A` now contains information about which points are near (within distance `epsilon`) which other points. We now pose the task of clustering the data points in `X` as the task of partitioning the rows and columns of `A`. 

Let $d_i = \sum_{j = 1}^n a_{ij}$ be the $i$th row-sum of $\mathbf{A}$, which is also called the *degree* of $i$. Let $C_0$ and $C_1$ be two clusters of the data points. We assume that every data point is in either $C_0$ or $C_1$. The cluster membership as being specified by `y`. We think of `y[i]` as being the label of point `i`. So, if `y[i] = 1`, then point `i` (and therefore row $i$ of $\mathbf{A}$) is an element of cluster $C_1$.  

The *binary norm cut objective* of a matrix $\mathbf{A}$ is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 
- $\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$ is the *cut* of the clusters $C_0$ and $C_1$. 
- $\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$, where $d_i = \sum_{j = 1}^n a_{ij}$ is the *degree* of row $i$ (the total number of all other rows related to row $i$ through $A$). The *volume* of cluster $C_0$ is a measure of the size of the cluster. 

A pair of clusters $C_0$ and $C_1$ is considered to be a "good" partition of the data when $N_{\mathbf{A}}(C_0, C_1)$ is small. To see why, let's look at each of the two factors in this objective function separately. 


#### B.1 The Cut Term

First, the cut term $\mathbf{cut}(C_0, C_1)$ is the number of nonzero entries in $\mathbf{A}$ that relate points in cluster $C_0$ to points in cluster $C_1$. Saying that this term should be small is the same as saying that points in $C_0$ shouldn't usually be very close to points in $C_1$. 

To do this, we will iterate thorugh each row and column in A, which is a 200x200 matrix, and check if the values in y[row] and y[column] match (either 0 or 1). If the points in y dont match, you then precede to check whether A at that index is equal to 1. If it is, it signifies that the point connections don't match (i.e they are in different clusters) and we add 1 to the cut term.

```python
def cut(A, y):
    
    cut = 0

    for row in range(A.shape[0]):
        for col in range(A.shape[1]):
            if y[row] != y[col]: #if the points in y dont match, check the value of A at that index
                if A[row][col] == 1: #if A at that index is equal to 1, add to cut term
                    cut += 1
                      
                
    return cut
```
cut(A, y)

This cut is 26. Not bad! This means that out of all the connections between points, only 26 of them have been cut. 


Compute the cut objective for the true clusters `y`. Then, generate a random vector of random labels of length `n`, with each label equal to either 0 or 1. Check the cut objective for the random labels. You should find that the cut objective for the true labels is *much* smaller than the cut objective for the random labels. 

This shows that this part of the cut objective indeed favors the true clusters over the random ones. 

```python
vector = np.random.randint(0, 2, size = n)

K = cut(A, vector)
K
```
The cut for randomly generated data is 2232, which is much higher, signifying over 2000 connections will be cut.

#### B.2 The Volume Term 

Now take a look at the second factor in the norm cut objective. This is the *volume term*. As mentioned above, the *volume* of cluster $C_0$ is a measure of how "big" cluster $C_0$ is. If we choose cluster $C_0$ to be small, then $\mathbf{vol}(C_0)$ will be small and $\frac{1}{\mathbf{vol}(C_0)}$ will be large, leading to an undesirable higher objective value. 

Synthesizing, the binary normcut objective asks us to find clusters $C_0$ and $C_1$ such that:

1. There are relatively few entries of $\mathbf{A}$ that join $C_0$ and $C_1$. 
2. Neither $C_0$ and $C_1$ are too small. 

Write a function called `vols(A,y)` which computes the volumes of $C_0$ and $C_1$, returning them as a tuple. For example, `v0, v1 = vols(A,y)` should result in `v0` holding the volume of cluster `0` and `v1` holding the volume of cluster `1`. Then, write a function called `normcut(A,y)` which uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of a matrix `A` with clustering vector `y`. 

Now, compare the `normcut` objective using both the true labels `y` and the fake labels you generated above. What do you observe about the normcut for the true labels when compared to the normcut for the fake labels? 

To start, the volume term protects against the solution where cluster 1 has all the points and cluster 0 has none. In other words, it trys to maintain a relatively balanced number of points per cluster. The cut term, as we saw above, was the number of data points that were sorted into different clusters C1 and C0. Here, we are calculating the size of the cluster, which is how many pairs of points are both in that specific cluster AND close together.

```python
def vols(A, y):
    C0 = np.where(y == 0)[0] #create an array of all indicies where y == 0
    C1 = np.where(y == 1)[0]
    
    v0 = A[C0].sum() #sum each row of A according to C0 index
    v1 = A[C1].sum()
    
    return(v0, v1)

def normcut(A, y):
    
    (v0, v1) = vols(A, y)
    c = cut(A, y)
    
    return (c*(1/v0+1/v1))

vols(A, y), vols(A, vector)
(normcut(A, y), normcut(A, vector))
```

As you can see, The normcut values according to the actual data compared to randomly generated points is much lower

## Part C

We have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in $A$ and (b) not too small. One approach to clustering is to try to find a cluster vector `y` such that `normcut(A,y)` is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick! 

Here's the trick: define a new vector $\mathbf{z} \in \mathbb{R}^n$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


Note that the signs of  the elements of $\mathbf{z}$ contain all the information from $\mathbf{y}$: if $i$ is in cluster $C_0$, then $y_i = 0$ and $z_i > 0$. 

Next, if you like linear algebra, you can show that 

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$
where $\mathbf{D}$ is the diagonal matrix with nonzero entries $d_{ii} = d_i$, and  where $d_i = \sum_{j = 1}^n a_i$ is the degree (row-sum) from before.  

1. Write a function called `transform(A,y)` to compute the appropriate $\mathbf{z}$ vector given `A` and `y`, using the formula above. 
2. Then, check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal. 
3. While you're here, also check the identity $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$, where $\mathbb{1}$ is the vector of `n` ones (i.e. `np.ones(n)`). This identity effectively says that $\mathbf{z}$ should contain roughly as many positive as negative entries. 

```python
def transform(A, y):
    
    (v0, v1) = vols(A, y)
    
    z = 1/v0*(y == 0) + -1/v1*(y == 1) #if element in y is equal to 0, use the positive volume notation. Otherwise, use - 1/v1
    
    return z
```

```python
def transform(A, y):
    
    (v0, v1) = vols(A, y)
    
    z = 1/v0*(y == 0) + -1/v1*(y == 1) #if element in y is equal to 0, use the positive volume notation. Otherwise, use - 1/v1
    
    return z
```

```python
np.isclose(0, z.T@D@np.ones(n)) #checks that the identity is close to zero
```








Awesome! It works.

### ¬ß3. Write a Geographic Scatter Function for Yearly Temperature Increases

Now we want to create visualizations that address the following question:

> How does the average yearly change in temperature vary within a given country?

We are going to write a function called `temperature_coefficient_plot()`. This function accepts five explicit arguments, and an undetermined number of keyword arguments.

- `country`, `year_begin`, `year_end`, and `month` should be as in the previous part.
- `min_obs`, the minimum required number of years of data for any given station. Only data for stations with at least `min_obs` years worth of data in the specified month should be plotted; the others should be filtered out. `df.transform()` plus filtering is a good way to achieve this task.
- `**kwargs`, additional keyword arguments passed to `px.scatter_mapbox()`. These can be used to control the colormap used, the mapbox style, etc.

The output of this function should be an interactive geographic scatterplot, constructed using Plotly Express, with a point for each station, such that the color of the point reflects an estimate of the yearly change in temperature during the specified month and time period at that station.

After writing our function, we will be able to create a plot of estimated yearly increases in temperature during the month of January, in the interval 1980-2020, in India, as follows:

```python
# assumes you have imported necessary packages
color_map = px.colors.diverging.RdGy_r # choose a colormap

fig = temperature_coefficient_plot("India", 1980, 2020, 1,
                                   min_obs = 10,
                                   zoom = 2,
                                   mapbox_style="carto-positron",
                                   color_continuous_scale=color_map)

fig.show()
```

OK let's get started.
If we want to estimate the yearly increase in temperature for a given station at a given time, we will want to use a Linear Regression model. Let's create a quick function that will take in the Temperature and Year as x and y and produce a LR coefficient for us. We will later add this data in a column in our data frame.

```python
def coef(data):
    x = data[["Year"]]
    y = data["Temp"]
    LR = LinearRegression()
    LR.fit(x, y)
    return LR.coef_[0] #returning Linear Reg coef
```

We want to use our Query function from part one to gather the data needed. Since there is an additional parameter `min_obs`, we want to count how many measures were taken at a particular station in a certain year and filter them out.
To do this, we will group by the station `NAME` and the `Year`, and use the transform function to produce the length, or number of temperature readings for that station during that particular year:

```python
min_ob = df.groupby(["NAME"])["Year"].transform(len)
```

Then we will filter out the stations that don't have the minimum number of temperature readings from our dataset:

```python
 df = df[min_ob >= min_obs] #filtering out all countries with less than the minimum number of observations
```

After doing this, we will use the apply function to apply the linear regression coefficient to the dataframe:

```python
data = df.groupby(["NAME", "LATITUDE", "LONGITUDE"]).apply(coef)
```

We'll reset the index, rename the columns, and round the data of the new `Estimated Annual Increase (C)` column to three decimal places:

```python
data = data.reset_index()
data = data.rename(columns = {0 : "Estimated Annual Increase (C)"}) #renames column
data["Estimated Annual Increase (C)"] = data["Estimated Annual Increase (C)"].round(3) #rounds decimals in column
```
Since our months are also in numeric format, we will create a dictionary to help with string translation in the future.
```python
monthw = {1: "January", 2:"February", 3: "March", 4: "April", 5: "May", 6: "June", 7: "July", 8: "August", 9: "September", 10: "October", 11: "November", 12: "December"}
   #month dictionary
```
Now, we will create our figure using plotly's px.scatter_mapbox() using our updated data, `LATITUDE` and `LONGITUDE`, Station `NAME`, and code by color:

```python
fig = px.scatter_mapbox(data,
                           lat = "LATITUDE",
                           lon = "LONGITUDE",
                           hover_name = "NAME",
                           hover_data = ["Estimated Annual Increase (C)"],
                           color = "Estimated Annual Increase (C)",
                           title = "Estimates of annual increase in temperature in " + monthw[month] + f" for stations in {country}, years {year_begin} - {year_end}", **kwargs)
    return fig
```

This is what our full function definition looks like:

```python
def temperature_coefficient_plot(country, year_begin, year_end, month, min_obs, **kwargs):

    df = query_climate_database(country, year_begin, year_end, month)

    min_ob = df.groupby(["NAME"])["Year"].transform(len)

    df = df[min_ob >= min_obs] #filtering out all countries with less than the minimum number of observations

    data = df.groupby(["NAME", "LATITUDE", "LONGITUDE"]).apply(coef) #applying linear regresison coef to df
    data = data.reset_index()
    data = data.rename(columns = {0 : "Estimated Annual Increase (C)"}) #renames column
    data["Estimated Annual Increase (C)"] = data["Estimated Annual Increase (C)"].round(3) #rounds decimals in column

    monthw = {1: "January", 2:"February", 3: "March", 4: "April", 5: "May", 6: "June", 7: "July", 8: "August", 9: "September", 10: "October", 11: "November", 12: "December"}
    #month dictionary
    fig = px.scatter_mapbox(data,
                           lat = "LATITUDE",
                           lon = "LONGITUDE",
                           hover_name = "NAME",
                           hover_data = ["Estimated Annual Increase (C)"],
                           color = "Estimated Annual Increase (C)",
                           title = "Estimates of annual increase in temperature in " + monthw[month] + f" for stations in {country}, years {year_begin} - {year_end}", **kwargs)
    return fig
```
Let's try it out.

```python
color_map = px.colors.diverging.RdGy_r # choose a colormap

fig = temperature_coefficient_plot("India", 1980, 2020, 1,
                                   min_obs = 10,
                                   zoom = 2,
                                   mapbox_style="carto-positron",
                                   color_continuous_scale=color_map)

fig.show()
```
{% include india-HW0-example.html %}

### ¬ß4. Temperature Anomalies by Country

> How have the frequency of temperature anomalies changed across different countries?

We want to create a visualization that compares three countries and looks at the frequency of temperature anomalies during a certain month each year within a certain time frame. To do this, we want to define a function `temp_anom()` that accepts

- `country 1`: the first country we want to look at
- `country 2`: the second country we want to look at
- `country 3`: the third country we want to look at
- `year_begin`: the year we want to start looking at
- `year_end`: the year we want to end at
- `month`: the specific month we want to look at

We want to define a function that will allow us to detect temperature anomalies by looking at z-score values.
```python
def z_score(x):
    m = np.mean(x)
    s = np.std(x)
    return (x-m)/s
```

Now that we have this, lets define our function:

```python
def temp_anom(country1, country2, country3, year_begin, year_end, month):
    country1 = query_climate_database(country1, year_begin, year_end, month)
    country2 = query_climate_database(country2, year_begin, year_end, month)
    country3 = query_climate_database(country3, year_begin, year_end, month)

    df = pd.concat([country1, country2, country3], keys=[country1, country2, country3],
    names=['Country', 'Index'], ignore_index=True)

    df["Z-Score"] = df.groupby(["NAME", "Month"])["Temp"].transform(z_score)

    anomalies = df[np.abs(df["Z-Score"]) > 2]

    anomalies = anomalies.groupby(["Name", "Year"])["Z-Score"].count()
    anomalies = anomalies.to_frame()
    anomalies = anomalies.sort_index(ascending = True)
    anomalies = anomalies.reset_index()
    anomalies = anomalies.rename(columns = {"Z-Score" : "Temp_Anomalies"})

    monthw = {1: "January", 2:"February", 3: "March", 4: "April", 5: "May", 6: "June", 7: "July", 8: "August", 9: "September", 10: "October", 11: "November", 12: "December"}

    fig = px.line(data_frame = anomalies,
                  x = "Year",
                  y = "Temp_Anomalies",
                  title = "Number of Temperature Anomalies in " + monthw[month] + " between " + str(year_begin) +" and " + str(year_end),
                  width = 1200,
                  height = 800,
                  color = "Name")

    return fig
```
Breaking this down:
1. We extracted data using our query function for each country.
2. We concatenated this data in an all-encompassing data frame so that we have all the info from each country during a given time period:
```python
df = pd.concat([country1, country2, country3], keys=[country1, country2, country3],
names=['Country', 'Index'], ignore_index=True)
```
3. We used our z-score function to create a column in the data frame that stores z-scores for each occurrence:
```python
df["Z-Score"] = df.groupby(["NAME", "Month"])["Temp"].transform(z_score)
```
4. We filtered out the regularities and preserved the anomalies in a data frame by checking whether absolute values of Z-Score were greater than 2:
```python
anomalies = df[np.abs(df["Z-Score"]) > 2]
```
5. We grouped by `Name` and `Year` and used the `count()` function to determine how many anomalies there were for each year, and we cleaned our anomalies data frame so that it is easier to read by sorting, resetting indices and renaming columns.
```python
anomalies = anomalies.groupby(["Name", "Year"])["Z-Score"].count()
anomalies = anomalies.to_frame()
anomalies = anomalies.sort_index(ascending = True)
anomalies = anomalies.reset_index()
anomalies = anomalies.rename(columns = {"Z-Score" : "Temp_Anomalies"}) #renaming to demonstrate number or temp anomalies
```
6. We created a dictionary to supplement the month column of the data frame to facilitation integer to string translation for our graph title:
```python
monthw = {1: "January", 2:"February", 3: "March", 4: "April", 5: "May", 6: "June", 7: "July", 8: "August", 9: "September", 10: "October", 11: "November", 12: "December"}
```
7. We created a line graph with the anomalies data frame, with `Year` on the x-axis and the `Temp_Anomalies`, or the Number of Temperature Anomalies on the y-axis. We defined a title, figure width and height, and corresponding colors.
```python
fig = px.line(data_frame = anomalies,
              x = "Year",
              y = "Temp_Anomalies",
              title = "Number of Temperature Anomalies in " + monthw[month] + " between " + str(year_begin) +" and " + str(year_end),
              width = 800,
              height = 500,
              color = "Name")
```
Let's try it out:

```python
fig = temp_anom(country1 = "United States", country2 = "India", country3 = "Australia",
                       year_begin = 1990,
                       year_end = 2020,
                       month = 1)

fig.show()
```
{% include anomaly_frequency.html %}

### ¬ß5. Temperature Variation within a Country Based on Latitude and Longitude

> How does the latitude and longitude affect temperature averages within a country?

We also wanted to look at the average temperature visualizations during a certain month across a designated number of years at differing latitudes and longitudes. To do this, we want to create another function that calculates the average temperatures at each station based on a minimum number of observations at differing latitudes and longitudes and graphs it in a 3-D plot.

Let's start:

1. Let's user our Query function to obtain a database for a certain location

2. We will filter the data based on our minimum number of observations to ensure that our averages aren't too skewed

3. We will group by `NAME` and `MONTH` and compute the average temperature for that particular year

4. We will reset the index and create our month dictionary to facilitate integer to string conversion

5. We will plot our data using a 3-D scatterplot and color based on temperature variation.
This way, we can see which stations at certain latitudes and longitudes have higher/lower temperatures at a given month

```python
def location_temp(country, year_begin, year_end, month, min_obs, **kwargs):

    df = query_climate_database(country, year_begin, year_end, month)

    min_ob = df.groupby(["NAME"])["Year"].transform(len) #finding the number of observation occurrences

    df = df[min_ob >= min_obs] #filtering out all countries with less than the minimum number of observations

    df["avg"] = df.groupby(["NAME", "Month"])["Temp"].transform(np.mean) #applying mean to data
    df = df.reset_index()

    monthw = {1: "January", 2:"February", 3: "March", 4: "April", 5: "May", 6: "June", 7: "July", 8: "August", 9: "September", 10: "October", 11: "November", 12: "December"}

    fig = px.scatter_3d(df,
                   x = "LATITUDE",
                   y = "LONGITUDE",
                   z = "avg",
                   color = "avg",
                   hover_name = "NAME",
                   opacity = 0.5,
                   title = "Average temperatures based on Latitudes + Longitudes in " + monthw[month] + f" for stations in {country}, years {year_begin} - {year_end}")
    return fig
```
Let's plot!

```python

fig = location_temp(country = "India",
                       year_begin = 1980,
                       year_end = 2020,
                       month = 6,
                       min_obs = 40,
                       colorscale = 'Viridus')

fig.show()
```
{% include location_temp.html %}

Now we can see how latitude and longitude affect average temperatures in a certain country during a certain month.

{::options parse_block_html="true" /}
<div class="got-help">
Describe where you learned something from peer feedback
</div>
{::options parse_block_html="false" /}
{::options parse_block_html="true" /}
<div class="gave-help">
Gave feedback
</div>
{::options parse_block_html="false" /}
