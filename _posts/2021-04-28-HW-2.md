---
layout: post
title: Blog Post 2
---

In this blog post, we will write a tutorial on a simple version of the spectral clustering algorithm for clustering data points.

#### Notation

In all the math below:
- boldface capital letters like A refer to matrices (2d arrays of numbers).
- Boldface lowercase letters like  ùêØ refer to vectors (1d arrays of numbers).
- AB refers to a matrix-matrix product (`A@B`). Av refers to a matrix-vector product (`A@v`). 

### ¬ß1. Intro

In this problem, we'll study *spectral clustering*. Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure. To start, let's look at an example where we *don't* need spectral clustering. 

### Lets Get Started

First lets import all the packages we will need

```python
import numpy as np
from plotly import express as px
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt # optional, in case we want to plot
```

```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```
![Blobs]({{site.baseurl}}/images/blobs.png)

*Clustering* refers to the task of separating this data set into the two natural "blobs." K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these: 

```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![KMeans Blobs]({{site.baseurl}}/images/kmeansblobs.png)
### Harder Clustering

That was all well and good, but what if our data is "shaped weird"? 

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

![Making Moons]({{site.baseurl}}/images/moons.png)

We can still make out two meaningful clusters in the data, but now they aren't blobs but crescents. As before, the Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`. Now k-means won't work so well, because k-means is, by design, looking for circular clusters. 

```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![Kmeans Moons]({{site.baseurl}}/images/kmeansmoons.png)

Whoops! That's not right!
As we'll see, spectral clustering is able to correctly cluster the two crescents. In the following problems, you will derive and implement spectral clustering.

## Part A: Constructing a Similarity Matrix, A

We are going to construct a similarity matrix, using parameter `epsilon` as our threshod for cluster identification. Entry `A[i,j]` should be equal to `1` if `X[i]` (the coordinates of data point `i`) is within distance `epsilon` of `X[j]` (the coordinates of data point `j`). 

**The diagonal entries `A[i,i]` should all be equal to zero.** We are going to use epsilon = 0.4

This similarity matrix will detail the relationship of one point to others and show us the number of connections each point in a network of points has. Think of each row as representing a point, and each column representing a point. These columns show whether a two points are within distance epsilon from each other or not.

```python
epsilon = 0.4

A = 1.0*(sklearn.metrics.pairwise_distances(X) < epsilon) #if point is within distance epsilon, it is denoted as 1
np.fill_diagonal(A, 0) #fills the diagonal with values of A and everything else is 0
```

## Part B

The matrix `A` now contains information about which points are near (within distance `epsilon`) which other points. We now pose the task of clustering the data points in `X` as the task of partitioning the rows and columns of `A`. 

Let $d_i = \sum_{j = 1}^n a_{ij}$ be the $i$th row-sum of $\mathbf{A}$, which is also called the *degree* of $i$. Let $C_0$ and $C_1$ be two clusters of the data points. We assume that every data point is in either $C_0$ or $C_1$. The cluster membership as being specified by `y`. We think of `y[i]` as being the label of point `i`. So, if `y[i] = 1`, then point `i` (and therefore row $i$ of $\mathbf{A}$) is an element of cluster $C_1$.  

The *binary norm cut objective* of a matrix $\mathbf{A}$ is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

A pair of clusters `C_0` and `C_1` is considered to be a "good" partition of the data when `N(C0, C1)` is small. To see why, let's look at each of the two factors in this objective function separately. 


#### B.1 The Cut Term

First, the cut term `cut(C_0, C_1)` is the number of nonzero entries in A that relate points in cluster `C_0` to points in cluster `C_1`. Saying that this term should be small is the same as saying that points in `C_0` shouldn't usually be very close to points in `C_1`. 

To do this, we will iterate thorugh each row and column in A, which is a 200x200 matrix, and check if the values in y[row] and y[column] match (either 0 or 1). If the points in y dont match, you then precede to check whether A at that index is equal to 1. If it is, it signifies that the point connections don't match (i.e they are in different clusters) and we add 1 to the cut term.

```python
def cut(A, y):
    
    cut = 0

    for row in range(A.shape[0]):
        for col in range(A.shape[1]):
            if y[row] != y[col]: #if the points in y dont match, check the value of A at that index
                if A[row][col] == 1: #if A at that index is equal to 1, add to cut term
                    cut += 1
                      
                
    return cut
```
```python
cut(A, y)
```

This cut is 26. Not bad! This means that out of all the connections between points, only 26 of them have been cut. This is good, because we want to minimize the number of connections being cut in order to create a more accurate cluster prediction.


Now we will compute the cut objective for the true clusters `y`. Then, we will generate a random vector of random labels of length `n`, with each label equal to either 0 or 1. After checking the cut objective for the random labels, we should find that the cut objective for the true labels is *much* smaller than the cut objective for the random labels. 

This shows that this part of the cut objective indeed favors the true clusters over the random ones. 

```python
vector = np.random.randint(0, 2, size = n)

K = cut(A, vector)
K
```
The cut for randomly generated data is 2232, which is much higher, signifying over 2000 connections will be cut. This makes sense, because we are using random data instead of our true data (which has clusters).

#### B.2 The Volume Term 

Now take a look at the second factor in the norm cut objective. This is the *volume term*. As mentioned above, the *volume* of cluster `C_0` is a measure of how "big" cluster `C_0` is. If we choose cluster `C_0` to be small, then `vol C0` will be small and `1/vol C0` will be large, leading to an undesirable higher objective value. 

Synthesizing, the binary normcut objective asks us to find clusters `C_0` and `C_1` such that:

1. There are relatively few entries of `A` that join `C_0` and `C_1`. 
2. Neither `C_0` and `C_1` are too small. 

Now we are going to write a function called `vols(A,y)` which computes the volumes of `C_0` and `C_1`, returning them as a tuple. Then, we will write a function called `normcut(A,y)` which uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of a matrix `A` with clustering vector `y`. 

To start, the volume term protects against the solution where cluster 1 has all the points and cluster 0 has none. In other words, it trys to maintain a relatively balanced number of points per cluster. The cut term, as we saw above, was the number of data points that were sorted into different clusters C1 and C0. Here, we are calculating the size of the cluster, which is how many pairs of points are both in that specific cluster AND close together.

```python
def vols(A, y):
    C0 = np.where(y == 0)[0] #create an array of all indicies where y == 0
    C1 = np.where(y == 1)[0]
    
    v0 = A[C0].sum() #sum each row of A according to C0 index
    v1 = A[C1].sum()
    
    return(v0, v1)

def normcut(A, y):
    
    (v0, v1) = vols(A, y)
    c = cut(A, y)
    
    return (c*(1/v0+1/v1))

vols(A, y), vols(A, vector)
(normcut(A, y), normcut(A, vector))
```

As you can see, The normcut values according to the actual data compared to randomly generated points are much lower, indicating that less data is being 'cut' from the original cluster group.

## Part C

We have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in $A$ and (b) not too small. One approach to clustering is to try to find a cluster vector `y` such that `normcut(A,y)` is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick! 

Here's the trick: define a new vector $\mathbf{z} \in \mathbb{R}^n$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


Note that the signs of  the elements of $\mathbf{z}$ contain all the information from $\mathbf{y}$: if $i$ is in cluster $C_0$, then $y_i = 0$ and $z_i > 0$. 

1. We are going to write a function called `transform(A,y)` to compute the appropriate `z` vector given `A` and `y`, using the formula above. 
2. Then, we will check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal. 
3. While we're here, we will also check the identity $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$, where $\mathbb{1}$ is the vector of `n` ones (i.e. `np.ones(n)`). This identity effectively says that $\mathbf{z}$ should contain roughly as many positive as negative entries. 

To start, we will define a transform function, which takes in a matrix of data and its y target data. We will use the vols function that we previously defined to get the respective cluster sizes and then determine which operation to perform on each data point based on its corresponding y value. For instance, if y is equal to 0, then we will eecute 1/v0. However, if the corresponding y value is one, we will use negative volume notation. This will help us identify which points are in which cluster.

```python
def transform(A, y):
    
    (v0, v1) = vols(A, y)
    
    z = 1/v0*(y == 0) + -1/v1*(y == 1) #if element in y is equal to 0, use the positive volume notation. Otherwise, use - 1/v1
    
    return z
```

Here we will check that z contains roughly as many positive as negative entries by checking that the identity of `zD1 = 0` is the vetor of `n` ones.

```python
z = transform(A, y)
k = np.sum(A, axis=1) #summing along rows of A

D = np.diag(k) #diagonal of k
```

```python
np.isclose(0, z.T@D@np.ones(n)) #checks that the identity is close to zero
```
Cool! This checks out!

## Part D

In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$
 We can actually do this by substituting for `z`, the orthogonal complement of `z` relative to `D1`. We will use the minimize function from scipy.optimize to minimize the function `orth_obj` with respect to `z`

```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

As you can see here, we use the scipy.optimize function to minimize with respect to z. 

```python
import scipy.optimize

z_min = scipy.optimize.minimize(orth_obj, np.random.rand(n)).x
```

**Note**: there's a cheat going on here! We originally specified that the entries of `z` should take only one of two values (back in Part C), whereas now we're allowing the entries to have *any* value! This means that we are no longer exactly optimizing the normcut objective, but rather an approximation. This cheat is so common that deserves a name: it is called the *continuous relaxation* of the normcut problem. 

## Part E

Recall that, by design, only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`. Plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. 


```python
color = 1*(z_min < 0)

plt.scatter(X[:,0], X[:, 1], c = color)
```

This looks pretty good! However, one tradeoff is that this method is really slow to practically use. Now lets try using another method to cluster that is shorter and uses eigenvalues/vectors instead, called spectral clustering.

## Part F

Explicitly optimizing the orthogonal objective is  *way* too slow to be practical. If spectral clustering required that we do this each time, no one would use it. 

The reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called *spectral* clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices. 

Recall that what we would like to do is minimize the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $\mathbf{z}$, subject to the condition $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$. 

The Rayleigh-Ritz Theorem states that the minimizing $\mathbf{z}$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Why is this helpful? Well, $\mathbb{1}$ is actually the eigenvector with smallest eigenvalue of the matrix $\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$. 

> So, the vector $\mathbf{z}$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

We will construct the matrix $\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$, which is often called the (normalized) *Laplacian* matrix of the similarity matrix $\mathbf{A}$, and we will find the eigenvector corresponding to its second-smallest eigenvalue, naming it `z_eig`. Then, we will plot the data again, using the sign of `z_eig` as the color. 

```python
L = np.linalg.inv(D)@(D - A)

eigvalue = np.linalg.eig(L)[0]
eigvector = np.linalg.eig(L)[1]

idx = eigvalue.argsort()[1] #index of the second smallest element

z_eig = eigvector[:, idx] #getting the corresponding eigenvector
plot = z_eig < 0 #plotting based on positive and neg values
```

```python
plt.scatter(X[:,0], X[:, 1], c = plot)
```

We get basically the same result, and it was much quicker.

## Part G

Let's put it all together now. We will define a function that constructs a similarity matrix, then a Laplacian matrix, and it will also compute the eigenvector with the second-smallest eigenvalue of the Laplacian matrix, returning labels based on this eigenvector (0s or 1s).

This is basically an aggregation of most things we did above, so we will essentially just copy-paste with a few minimal changes.

```python
def spectral_clustering(X, epsilon):
    """
    Takes in data X in a matrix, and a float epsilon, which determines the threshold for the cluster
    Constructs a similarity matrix, a Laplacian matrix, and an eigenvector with the second smallest 
    eigenvalue of the Laplacian matrix.
    Returns labels based on this eigenvector encoded into either 0 or 1 based on if the data point in in cluster
    C1 or C0
    """
    S = 1.0*(sklearn.metrics.pairwise_distances(X) < epsilon) #if point is within distance epsilon, it is denoted as 1
    np.fill_diagonal(S, 0)
    
    k = np.sum(S, axis=1) #summing along rows of S 
    D = np.diag(k) #fills diagonal of D with row sums from k
    
    L = np.linalg.inv(D)@(D - S) #computes a Laplacian matrix
    
    eigenvalue, eigenvector = np.linalg.eig(L)[0], np.linalg.eig(L)[1] #stores the eigenvalues and eigenvectors in separate matrices

    idx = eigenvalue.argsort()[1] #index of the second smallest eigenvalue

    z_eig = eigenvector[:, idx] #getting the corresponding eigenvector
    
    return 1*(z_eig < 0) #if the eigenvector values are less than zero, they are in the cluster C1, indicated by a 1.
    #otherwise, they are in cluster C0, indicated by 0
```

## Part H

Run a few experiments using your function, by generating different data sets using `make_moons`. What happens when you increase the `noise`? Does spectral clustering still find the two half-moon clusters? For these experiments, you may find it useful to increase `n` to `1000` or so -- we can do this now, because of our fast algorithm! 

Lets make our moons!

```python
np.random.seed(1435)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.15, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

![Make Moons]({{site.baseurl}}/images/bluemoons.png)

Now let's test it out!

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.25))
````
![Using Spectral Clustering on Moons]({{site.baseurl}}/images/bluemoonssc.png)

## Part I

Now lets try our spectral clustering function on another data set -- the bull's eye! 

```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```
![Make Circles]({{site.baseurl}}/images/bluecircles.png)

There are two concentric circles. As before k-means will not do well here at all. 

```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![KMeans Circles]({{site.baseurl}}/images/kmeanscircles.png)

Lets see if our function can successfully separate the circles. We will try different values of epsilon.

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.3))
```
![Spectral Clustering Circles]({{site.baseurl}}/images/sccircles0.3.png)

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.2))
```
![Spectral Clustering Circles]({{site.baseurl}}/images/sccircles0.2.png)

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.31))
```
![Spectral Clustering Circles]({{site.baseurl}}/images/sccircles0.31.png)

Amazing! With Spectral Clustering we were able to somewhat accurately identify separate clusters of non-circluar shapes. By utlizing eigenvectors and eigenvalues, we were able to quickly identify the cut that optimizes both the size (volume) of clusters that cuts as few connections as possible.

{::options parse_block_html="true" /}
<div class="got-help">
Describe where you learned something from peer feedback
</div>
{::options parse_block_html="false" /}
{::options parse_block_html="true" /}
<div class="gave-help">
Gave feedback
</div>
{::options parse_block_html="false" /}
